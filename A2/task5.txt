
BLEU scores when n=1

      1K	10K	  15K	    30K
    0.4286    0.5714    0.6429    0.6429
    0.4286    0.5714    0.5714    0.5714
    0.5455    0.6364    0.6364    0.6364
    0.3333    0.5833    0.5833    0.5833
    0.6364    0.8182    0.8182    0.7273
    0.6250    0.6250    0.7500    0.7500
    0.3636    0.5455    0.6364    0.6364
    0.6667    0.8333    0.8333    1.0000
    0.6550    0.6550    0.6550    0.6550
    0.3750    0.3750    0.5000    0.5000
    0.8182    0.8182    0.6364    0.7273
    0.3636    0.4545    0.4545    0.4545
    0.2857    0.4286    0.4286    0.5714
    0.6250    0.6875    0.7500    0.8125
    0.5306    0.6822    0.7580    0.7580
    0.5000    0.7000    0.7000    0.7000
    0.5714    0.5714    0.5714    0.7143
    0.6275    0.6275    0.6798    0.7844
    0.4000    0.4000    0.5000    0.5000
    0.4412    0.6619    0.6619    0.6619
    0.6667    0.8333    0.8333    0.8333
    0.6250    0.8750    0.8750    0.8750
    0.7500    0.8333    0.8333    0.8333
    0.6154    0.6923    0.6923    0.6923
    0.3333    0.3333    0.3333    0.3333

BLEU scores when n=2

      1K	10K	  15K	    30K
    0.2928    0.3904    0.4140    0.4140
    0.2315    0.3780    0.3780    0.3780
    0.3015    0.3989    0.3989    0.3989
    0.2265    0.3669    0.3669    0.3669
    0.4606    0.6396    0.6396    0.6030
    0.5893    0.5893    0.6455    0.6455
    0.1741    0.3015    0.3989    0.3989
    0.5345    0.7715    0.7715    1.0000
    0.2990    0.2990    0.2990    0.2990
         0    0.2887    0.4082    0.4082
    0.5839    0.5839    0.5149    0.5505
    0.2462    0.3892    0.3892    0.3892
    0.1890    0.2315    0.2315    0.2673
    0.4697    0.4926    0.5145    0.5784
    0.3326    0.5333    0.6072    0.6072
    0.3693    0.6179    0.6179    0.6179
    0.4629    0.4629    0.4629    0.5976
    0.4979    0.4979    0.5497    0.6528
    0.2697    0.2697    0.3693    0.3693
    0.3603    0.6240    0.6240    0.6240
    0.5345    0.7715    0.7715    0.7715
    0.5270    0.8250    0.8250    0.6972
    0.4804    0.6202    0.6202    0.6202
    0.5547    0.5883    0.5883    0.5883
    0.3086    0.3086    0.3086    0.3086

BLEU scores when n=3

      1K	10K	  15K	    30K
    0.1829    0.2792    0.2904    0.2904
         0         0         0         0
         0         0         0         0
         0         0         0         0
         0    0.4206    0.4206    0.4044
    0.5069    0.5069    0.5386    0.5386
         0         0         0         0
         0    0.6677    0.6677    1.0000
         0         0         0         0
         0    0.2184    0.3467    0.3467
    0.3958    0.3958    0.3640    0.3805
         0    0.2397    0.2397    0.2397
         0         0         0         0
    0.3806    0.3929    0.4045    0.4373
    0.2031    0.3506    0.4376    0.4376
         0    0.4857    0.4857    0.4857
    0.3128    0.3128    0.3128    0.4673
    0.3729    0.3729    0.4291    0.5628
         0         0    0.2389    0.2389
    0.2428    0.5559    0.5559    0.5559
         0    0.6677    0.6677    0.6677
    0.3262    0.7521    0.7521    0.4953
    0.2679    0.4002    0.4002    0.4002
    0.4141    0.4740    0.4740    0.4307
         0         0         0         0

Firstly, among three values of n (i.e., n=1,2,3) in the BLEU score, obviously the first one with unigram has the highest BLEU score because it is
more difficult for the model to translate multiple consecutive words than the single word.And we even could some zeros in the 3-gram BLEU socres.
Therefore, the higher n, the lower BLEU socre.

As for the number of sentences trainied for four alignment models(1K,10K,15K and 30K), generally more training sentences would increase the performance
because there would be more new samples for training during EM process. Compared to 1K sentences, apparently 30K sentences contain more kinds of n-grams
and have a more accurate words distribution.
